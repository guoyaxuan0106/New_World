{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Model Training and Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9Va2JtIOomk0",
        "NPu-GjhgqnkY",
        "LLwXhQwCHIb3",
        "loGag7jyHM-1",
        "mjef7jONHOX6",
        "Gddi9-OWLkqT",
        "SCdA_a6PJBE2",
        "YcsBgmB6H6C6",
        "eWgvfAFLH_ov",
        "sMz3PcBUIGQ6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0d6949d484041d2b8ebada7c5701053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9201f66d66a4409abc617197985a4ec7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6cb34c558191415da8769a453988e1e8",
              "IPY_MODEL_d44c76838c47460daeaeb4dde81e81b8"
            ]
          }
        },
        "9201f66d66a4409abc617197985a4ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cb34c558191415da8769a453988e1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c0e17f7843b8405aae7544ebb002225b",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178728960,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178728960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e30b947ecfb48b7b5818df7ed81396a"
          }
        },
        "d44c76838c47460daeaeb4dde81e81b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_38a5e11387b6488384677175617aedfb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:01&lt;00:00, 106MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dc0a75f46fa4e2d89dfa1e6aaa9390f"
          }
        },
        "c0e17f7843b8405aae7544ebb002225b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e30b947ecfb48b7b5818df7ed81396a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38a5e11387b6488384677175617aedfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dc0a75f46fa4e2d89dfa1e6aaa9390f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK5ZR5XE5DhL"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og8AkMaDeE7A",
        "outputId": "8a386b61-0ca1-4d5f-a249-451640cf5869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjwWIfj4gaWi"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Video_Captioner/Data/train2014.zip'\n",
        "!unzip '/content/drive/My Drive/Video_Captioner/Data/val2014.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q129NFg2qD98",
        "outputId": "d0afe38b-aae6-4466-8013-acf0912a27d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "!wget http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
        "!unzip caption_datasets.zip\n",
        "!rm caption_datasets.zip\n",
        "!rm dataset_flickr30k.json\n",
        "!rm dataset_flickr8k.json"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-27 18:53:34--  http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip [following]\n",
            "--2020-10-27 18:53:34--  https://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36745453 (35M) [application/zip]\n",
            "Saving to: ‘caption_datasets.zip’\n",
            "\n",
            "caption_datasets.zi 100%[===================>]  35.04M  22.1MB/s    in 1.6s    \n",
            "\n",
            "2020-10-27 18:53:36 (22.1 MB/s) - ‘caption_datasets.zip’ saved [36745453/36745453]\n",
            "\n",
            "Archive:  caption_datasets.zip\n",
            "  inflating: dataset_coco.json       \n",
            "  inflating: dataset_flickr30k.json  \n",
            "  inflating: dataset_flickr8k.json   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Z5vtSSnnLN"
      },
      "source": [
        "# Datasets:\n",
        "#   train2014 is a folder of image files for training\n",
        "#   val2014 is a folder of image files for validation\n",
        "#   dataset_coco.json is a JSON file that tells you {image -> captions}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Va2JtIOomk0"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92LVl2M6q-NH"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "import cv2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78RzSgTmo1eL"
      },
      "source": [
        "## Data loading \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZwfFwbgo5qW",
        "outputId": "cdd2021c-fa93-47eb-d26e-2943bb0f40b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Load JSON file into dict\n",
        "json_path = 'dataset_coco.json'\n",
        "with open(json_path, 'r') as j:\n",
        "    data = json.load(j)\n",
        "print(data['images'][0])\n",
        "\n",
        "# Understand how each image is captioned\n",
        "# 'filename' is the image name\n",
        "# 'filepath' is the folder name\n",
        "# 'imgid' is the id of the image\n",
        "# 'sentences' is a list of the human captioning\n",
        "# 'tokens' is a list of words"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'filepath': 'val2014', 'sentids': [770337, 771687, 772707, 776154, 781998], 'filename': 'COCO_val2014_000000391895.jpg', 'imgid': 0, 'split': 'test', 'sentences': [{'tokens': ['a', 'man', 'with', 'a', 'red', 'helmet', 'on', 'a', 'small', 'moped', 'on', 'a', 'dirt', 'road'], 'raw': 'A man with a red helmet on a small moped on a dirt road. ', 'imgid': 0, 'sentid': 770337}, {'tokens': ['man', 'riding', 'a', 'motor', 'bike', 'on', 'a', 'dirt', 'road', 'on', 'the', 'countryside'], 'raw': 'Man riding a motor bike on a dirt road on the countryside.', 'imgid': 0, 'sentid': 771687}, {'tokens': ['a', 'man', 'riding', 'on', 'the', 'back', 'of', 'a', 'motorcycle'], 'raw': 'A man riding on the back of a motorcycle.', 'imgid': 0, 'sentid': 772707}, {'tokens': ['a', 'dirt', 'path', 'with', 'a', 'young', 'person', 'on', 'a', 'motor', 'bike', 'rests', 'to', 'the', 'foreground', 'of', 'a', 'verdant', 'area', 'with', 'a', 'bridge', 'and', 'a', 'background', 'of', 'cloud', 'wreathed', 'mountains'], 'raw': 'A dirt path with a young person on a motor bike rests to the foreground of a verdant area with a bridge and a background of cloud-wreathed mountains. ', 'imgid': 0, 'sentid': 776154}, {'tokens': ['a', 'man', 'in', 'a', 'red', 'shirt', 'and', 'a', 'red', 'hat', 'is', 'on', 'a', 'motorcycle', 'on', 'a', 'hill', 'side'], 'raw': 'A man in a red shirt and a red hat is on a motorcycle on a hill side.', 'imgid': 0, 'sentid': 781998}], 'cocoid': 391895}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7shV7op5JO8"
      },
      "source": [
        "# data[\"images\"][0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKGEnilkqXPf"
      },
      "source": [
        "# Each image may have multiple captions\n",
        "# to reduce the bias we are introducing, \n",
        "# let's use the same number of captions per image\n",
        "captions_per_image = 5\n",
        "\n",
        "# Maximum number of words in a sentence\n",
        "# If the sentence has more than max_len words, we skip it\n",
        "# If the sentence has less than max_len words, we pad it with <pad>\n",
        "max_len = 50\n",
        "\n",
        "# From json object to a list of (image_path, captions) pairs \n",
        "# note: captions should be a list of word lists\n",
        "train_img_cap_pairs = []\n",
        "val_img_cap_pairs = []\n",
        "test_img_cap_pairs = []\n",
        "\n",
        "# It contains all distinct words\n",
        "word_set = set()\n",
        "\n",
        "for img_obj in data['images']:\n",
        "    captions = []\n",
        "    for caption in img_obj['sentences']:\n",
        "        word_set.update(caption['tokens'])\n",
        "        if len(caption['tokens']) <= max_len:\n",
        "            captions.append(caption['tokens'])\n",
        "\n",
        "    # If captions is empty, what should we do here?\n",
        "    if len(captions) == 0:\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(img_obj['filepath'], img_obj['filename'])\n",
        "\n",
        "    # What if this image cannot be found?\n",
        "    if not os.path.exists(img_path): continue\n",
        "\n",
        "    # Append the pair to the list\n",
        "    if img_obj['split'] == 'train':\n",
        "      train_img_cap_pairs.append([img_path, captions])\n",
        "    elif img_obj['split'] == 'val':\n",
        "      val_img_cap_pairs.append([img_path, captions])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qay9wsqmvYFq"
      },
      "source": [
        "## Data tranformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpnovM8wvYfY"
      },
      "source": [
        "# HDF5: HDF5 is a unique technology suite that makes possible the management\n",
        "# of extremely large and complex data collections.\n",
        "\n",
        "# 1. We will create 2 hdf5 files: \n",
        "#      train_images.hdf5, val_images.hdf5\n",
        "# 2. We will create 5 json files: \n",
        "#      word_map.json -- contains a (word -> number) hash object\n",
        "#      train_captions.json -- contains a list of encoded training captions\n",
        "#      val_captions.json -- contains a list of encoded validation captions\n",
        "#      train_caption_length.json -- contains a list of training caption lengths\n",
        "#      val_caption_length.json -- contains a list of validation caption lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU4bEZ-Ww8Bu"
      },
      "source": [
        "# Word Encoding\n",
        "# word_map: word    -> number (starting from 1)\n",
        "#           <pad>   -> 0\n",
        "#           <start> -> the second highest number\n",
        "#           <end>   -> the highest number\n",
        "word_map = {k: idx + 1 for idx, k in enumerate(word_set)}\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0\n",
        "\n",
        "# Save word map to a JSON\n",
        "with open(os.path.join('word_map.json'), 'w') as j:\n",
        "  json.dump(word_map, j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgtQwvvTvyWO",
        "outputId": "10004c65-84df-4c2e-8fe9-7bc10d26368f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "for img_cap_pairs, split in [[train_img_cap_pairs,'train'], [train_img_cap_pairs, 'val']]:\n",
        "    # Save encoded captions and their lengths to JSON files\n",
        "    h5py_path = os.path.join(split + '_images.hdf5')\n",
        "    \n",
        "    # remove it if the path exists\n",
        "    if os.path.exists(h5py_path): os.remove(h5py_path)\n",
        "\n",
        "    with h5py.File(h5py_path, 'a') as h:\n",
        "        # Make a note of the number of captions we are sampling per image\n",
        "        h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "        # Create dataset inside HDF5 file to store images\n",
        "        # we do channel first for the image\n",
        "        images = h.create_dataset('images', (len(img_cap_pairs), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "        enc_captions = []\n",
        "        caplens = []\n",
        "        for index, img_cap_pair in enumerate(img_cap_pairs):\n",
        "            img_path, captions = img_cap_pair\n",
        "\n",
        "            if len(captions) < captions_per_image:\n",
        "                # add some captions by randomly sampling from captions\n",
        "                captions = captions + [choice(captions) for _ in range(captions_per_image - len(captions))]\n",
        "            else:\n",
        "                # randomly sample k from captions\n",
        "                captions = sample(captions, captions_per_image)\n",
        "\n",
        "            # Sanity check\n",
        "            assert len(captions) == captions_per_image\n",
        "\n",
        "            # Read image and transform it into (3, 256, 256)\n",
        "            # Hint: use cv2, you will need to read, resize and transpose\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (256, 256))\n",
        "            img = img.transpose(2, 0, 1)\n",
        "\n",
        "            assert img.shape == (3, 256, 256)\n",
        "\n",
        "            # Save image to HDF5 file\n",
        "            images[index] = img\n",
        "            for idx, caption in enumerate(captions):\n",
        "                # Encode captions\n",
        "                #   a list of numbers\n",
        "                #   Format should be <start> word1 word2 ... wordN <end> <pad> <pad>...\n",
        "                #   The total length should be equal to max_len\n",
        "                enc_c = [word_map['<start>']] + [word_map[word] for word in caption] + \\\n",
        "                 [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(caption))\n",
        "\n",
        "                enc_captions.append(enc_c)\n",
        "                caplens.append(len(caption) + 2)                \n",
        "\n",
        "    with open(os.path.join(split + '_captions.json'), 'w') as j:\n",
        "        json.dump(enc_captions, j)\n",
        "\n",
        "    with open(os.path.join(split + '_caption_length.json'), 'w') as j:\n",
        "        json.dump(caplens, j)\n",
        "\n",
        "# Sanity check\n",
        "print('caption length:', caplens[0])\n",
        "print('caption:', caption)\n",
        "print('caption encoding:', enc_c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "caption length: 10\n",
            "caption: ['a', 'sandwich', 'is', 'loaded', 'and', 'served', 'with', 'fries']\n",
            "caption encoding: [27930, 6376, 4324, 13954, 15178, 25809, 13074, 5354, 26667, 27931, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPu-GjhgqnkY"
      },
      "source": [
        "#Helper functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br2WmVaQl8zd"
      },
      "source": [
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
        "    if is_best:\n",
        "        torch.save(state, 'BEST_' + filename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fnKTTtm5Ru2"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLwXhQwCHIb3"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXd4HvZ0qmw2"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        # pretrained ImageNet ResNet-101\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)   \n",
        "\n",
        "        # Remove linear and pool layers. \n",
        "        # 1. Pooling layer:\n",
        "        #    Since the default average pooling will output a 100*100 image,\n",
        "        #    and if we want to customize the output image embedding,\n",
        "        #    then we should delete the default average pooling layer,\n",
        "        #    and use AdaptiveAvgPool2d to customize it\n",
        "        #    https://arxiv.org/pdf/1512.03385.pdf\n",
        "        # 2. Fully connected layer: \n",
        "        #    help to analyze information, but not extract information\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        # We want to avoid training encoder\n",
        "        # Only avoid resnet since adaptive_pool should still be trained\n",
        "        # 1. When could we use pre-trained CNN?\n",
        "        #    When the dataset used in pre-trained CNN is similar with the dataset we use\n",
        "\n",
        "        for param in self.resnet.parameters():\n",
        "          param.requires_grad = False\n",
        "        \n",
        "\n",
        "    def forward(self, images):\n",
        "        # Use resnet, apply adaptive_pool\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loGag7jyHM-1"
      },
      "source": [
        "##Attention "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd2Gpxu0HP2h"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjef7jONHOX6"
      },
      "source": [
        "##Decoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N6-RqyLHUvt"
      },
      "source": [
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gddi9-OWLkqT"
      },
      "source": [
        "# Pytoch dataset transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egEdtSr267C7"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        \"\"\"\n",
        "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(split + '_images.hdf5', 'r')\n",
        "        self.imgs = self.h['images']\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']\n",
        "\n",
        "        # Load encoded captions \n",
        "        self.captions = json.load(open(split + '_captions.json', 'r'))\n",
        "\n",
        "        # Load caption lengths\n",
        "        self.caplens = json.load(open(split + '_caption_length.json','r'))\n",
        "        \n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = len(self.caplens)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        # normalize pixels to [0,1]\n",
        "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        img = transforms.Compose([normalize])(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'train':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh54sUaU5aUk"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCdA_a6PJBE2"
      },
      "source": [
        "##Initialize parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZTWV83qq6dy"
      },
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Model hyper-parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
        "cudnn.benchmark = True  \n",
        "\n",
        "epochs = 30  \n",
        "batch_size = 32\n",
        "decoder_lr = 4e-4  \n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention'\n",
        "\n",
        "best_bleu4 = 0.  # BLEU-4 score right now"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcsBgmB6H6C6"
      },
      "source": [
        "## Training per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRS2xtHsH5nu"
      },
      "source": [
        "def train(train_loader, encoder, decoder, loss_function, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param loss_function: loss layer\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    # Load by batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        # Remember to use GPU\n",
        "        # Put all training datasets to GPU\n",
        "        # Since there will be a series of simple computations during image processing \n",
        "        # MLP (in one layer): parallel computing\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Encoding\n",
        "        encoded_imgs = encoder.forward(imgs)\n",
        "        # Decoding\n",
        "        preds, caps_sorted, decode_lengths, alphas, sort_ind = decoder.forward(encoded_imgs, caps, caplens)\n",
        "\n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        preds, _, _, _ = pack_padded_sequence(preds, decode_lengths, batch_first=True)\n",
        "        targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_function(preds, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Back prop on decoder only\n",
        "        loss.backward()\n",
        "        decoder_optimizer.step()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(accuracy(preds, targets, 5), sum(decode_lengths))\n",
        "\n",
        "        # Print status\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          loss=losses,\n",
        "                                                                          top5=top5accs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWgvfAFLH_ov"
      },
      "source": [
        "## Validation per epoch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUWGxZZ3H_z_"
      },
      "source": [
        "def validate(val_loader, encoder, decoder, loss_function):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param loss_function: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # eval mode (no dropout or batchnorm)\n",
        "    decoder.eval()  \n",
        "    encoder.eval()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            encoded_imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoded_imgs, caps, caplens)\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "\n",
        "            scores, _, _, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader),\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads\n",
        "                references.append(img_captions)\n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMz3PcBUIGQ6"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehs0beBuq_mM",
        "outputId": "24d74e7f-a0b3-4a6d-96b7-9b67a3472813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b0d6949d484041d2b8ebada7c5701053",
            "9201f66d66a4409abc617197985a4ec7",
            "6cb34c558191415da8769a453988e1e8",
            "d44c76838c47460daeaeb4dde81e81b8",
            "c0e17f7843b8405aae7544ebb002225b",
            "7e30b947ecfb48b7b5818df7ed81396a",
            "38a5e11387b6488384677175617aedfb",
            "4dc0a75f46fa4e2d89dfa1e6aaa9390f"
          ]
        }
      },
      "source": [
        "# Read word map\n",
        "word_map_file = 'word_map.json'\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "\n",
        "# Initialize encoder, and we don't train it\n",
        "encoder = Encoder()\n",
        "encoder_optimizer = None\n",
        "\n",
        "# Initialize decoder, and adam optimizer\n",
        "# Why is len(word_map) here?\n",
        "decoder = DecoderWithAttention(attention_dim, emb_dim, decoder_dim, len(word_map), dropout=dropout)\n",
        "decoder_optimizer = torch.optim.Adam(\n",
        "    params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "    lr=decoder_lr\n",
        ")\n",
        "\n",
        "\n",
        "# Move to GPU, if available\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "# Why use CrossEntropyLoss():\n",
        "#   Model predict + softmax: normalize and obtain the probability of each word\n",
        "#   CrossEntropyLoss: loss function to help gradient tends to optimal values \n",
        "loss_function = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Custom dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = CaptionDataset('train'), \n",
        "    batch_size = batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = 1\n",
        "    )\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset = CaptionDataset('val'), \n",
        "    batch_size = batch_size, \n",
        "    shuffle = False, \n",
        "    pin_memory = True,\n",
        "    num_workers = 1\n",
        "    )\n",
        "\n",
        "# Epochs\n",
        "for epoch in range(epochs):\n",
        "    # One epoch's training\n",
        "    train(train_loader=train_loader,\n",
        "          encoder=encoder,\n",
        "          decoder=decoder,\n",
        "          loss_function=loss_function,\n",
        "          decoder_optimizer = decoder_optimizer,\n",
        "          epoch = epoch)\n",
        "\n",
        "    # One epoch's validation\n",
        "    recent_bleu4 = validate(val_loader=val_loader,\n",
        "                            encoder=encoder,\n",
        "                            decoder=decoder,\n",
        "                            loss_function = loss_function)\n",
        "\n",
        "    # Save checkpoint\n",
        "    # Automate save the training results and restart the traning from that epoch\n",
        "    # Split training and testing, testing could directly start using parameters of one parameter\n",
        "    is_best = recent_bleu4 > best_bleu4\n",
        "    best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "\n",
        "    save_checkpoint('coco', epoch, 0, encoder, decoder, encoder_optimizer,\n",
        "                    decoder_optimizer, recent_bleu4, is_best)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0d6949d484041d2b8ebada7c5701053",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: [0][0/126]\tLoss 11.1872 (11.1872)\tTop-5 Accuracy 0.000 (0.000)\n",
            "Epoch: [0][100/126]\tLoss 6.2294 (6.9914)\tTop-5 Accuracy 37.598 (33.886)\n",
            "Validation: [0/126]\tLoss 6.3415 (6.3415)\tTop-5 Accuracy 38.786 (38.786)\t\n",
            "Validation: [100/126]\tLoss 5.8691 (5.6873)\tTop-5 Accuracy 36.585 (41.554)\t\n",
            "\n",
            " * LOSS - 5.682, TOP-5 ACCURACY - 41.598, BLEU-4 - 0.01102232766415288\n",
            "\n",
            "Epoch: [1][0/126]\tLoss 5.9238 (5.9238)\tTop-5 Accuracy 38.630 (38.630)\n",
            "Epoch: [1][100/126]\tLoss 5.6147 (5.5995)\tTop-5 Accuracy 44.757 (44.147)\n",
            "Validation: [0/126]\tLoss 5.7665 (5.7665)\tTop-5 Accuracy 46.174 (46.174)\t\n",
            "Validation: [100/126]\tLoss 5.2657 (5.0674)\tTop-5 Accuracy 48.509 (51.094)\t\n",
            "\n",
            " * LOSS - 5.059, TOP-5 ACCURACY - 51.141, BLEU-4 - 0.08360096984917317\n",
            "\n",
            "Epoch: [2][0/126]\tLoss 4.9738 (4.9738)\tTop-5 Accuracy 54.839 (54.839)\n",
            "Epoch: [2][100/126]\tLoss 5.1145 (5.1009)\tTop-5 Accuracy 49.304 (50.691)\n",
            "Validation: [0/126]\tLoss 5.4035 (5.4035)\tTop-5 Accuracy 48.549 (48.549)\t\n",
            "Validation: [100/126]\tLoss 4.8775 (4.6765)\tTop-5 Accuracy 52.304 (56.453)\t\n",
            "\n",
            " * LOSS - 4.668, TOP-5 ACCURACY - 56.507, BLEU-4 - 0.10676503956318703\n",
            "\n",
            "Epoch: [3][0/126]\tLoss 4.4915 (4.4915)\tTop-5 Accuracy 59.487 (59.487)\n",
            "Epoch: [3][100/126]\tLoss 4.7435 (4.7559)\tTop-5 Accuracy 53.264 (55.263)\n",
            "Validation: [0/126]\tLoss 5.0383 (5.0383)\tTop-5 Accuracy 52.770 (52.770)\t\n",
            "Validation: [100/126]\tLoss 4.5624 (4.3812)\tTop-5 Accuracy 57.453 (60.309)\t\n",
            "\n",
            " * LOSS - 4.373, TOP-5 ACCURACY - 60.372, BLEU-4 - 0.13240415592394478\n",
            "\n",
            "Epoch: [4][0/126]\tLoss 4.3726 (4.3726)\tTop-5 Accuracy 60.714 (60.714)\n",
            "Epoch: [4][100/126]\tLoss 4.5881 (4.5221)\tTop-5 Accuracy 58.856 (58.327)\n",
            "Validation: [0/126]\tLoss 4.7788 (4.7788)\tTop-5 Accuracy 55.145 (55.145)\t\n",
            "Validation: [100/126]\tLoss 4.2943 (4.1335)\tTop-5 Accuracy 59.621 (63.377)\t\n",
            "\n",
            " * LOSS - 4.126, TOP-5 ACCURACY - 63.380, BLEU-4 - 0.15668317395129716\n",
            "\n",
            "Epoch: [5][0/126]\tLoss 4.4958 (4.4958)\tTop-5 Accuracy 55.145 (55.145)\n",
            "Epoch: [5][100/126]\tLoss 4.4601 (4.3196)\tTop-5 Accuracy 57.808 (60.662)\n",
            "Validation: [0/126]\tLoss 4.5625 (4.5625)\tTop-5 Accuracy 56.464 (56.464)\t\n",
            "Validation: [100/126]\tLoss 4.0960 (3.9330)\tTop-5 Accuracy 60.163 (65.543)\t\n",
            "\n",
            " * LOSS - 3.927, TOP-5 ACCURACY - 65.625, BLEU-4 - 0.17066559111650823\n",
            "\n",
            "Epoch: [6][0/126]\tLoss 4.0031 (4.0031)\tTop-5 Accuracy 64.499 (64.499)\n",
            "Epoch: [6][100/126]\tLoss 4.3651 (4.1400)\tTop-5 Accuracy 58.974 (62.621)\n",
            "Validation: [0/126]\tLoss 4.3431 (4.3431)\tTop-5 Accuracy 59.894 (59.894)\t\n",
            "Validation: [100/126]\tLoss 3.8918 (3.7386)\tTop-5 Accuracy 63.957 (67.988)\t\n",
            "\n",
            " * LOSS - 3.732, TOP-5 ACCURACY - 68.181, BLEU-4 - 0.1978602496691239\n",
            "\n",
            "Epoch: [7][0/126]\tLoss 4.0371 (4.0371)\tTop-5 Accuracy 66.667 (66.667)\n",
            "Epoch: [7][100/126]\tLoss 3.8572 (3.9728)\tTop-5 Accuracy 67.033 (64.927)\n",
            "Validation: [0/126]\tLoss 4.1442 (4.1442)\tTop-5 Accuracy 62.797 (62.797)\t\n",
            "Validation: [100/126]\tLoss 3.7221 (3.5869)\tTop-5 Accuracy 64.770 (69.946)\t\n",
            "\n",
            " * LOSS - 3.581, TOP-5 ACCURACY - 70.050, BLEU-4 - 0.2084651219224989\n",
            "\n",
            "Epoch: [8][0/126]\tLoss 3.6224 (3.6224)\tTop-5 Accuracy 70.960 (70.960)\n",
            "Epoch: [8][100/126]\tLoss 3.8433 (3.8151)\tTop-5 Accuracy 65.455 (66.916)\n",
            "Validation: [0/126]\tLoss 3.9818 (3.9818)\tTop-5 Accuracy 62.797 (62.797)\t\n",
            "Validation: [100/126]\tLoss 3.5317 (3.4245)\tTop-5 Accuracy 67.209 (72.139)\t\n",
            "\n",
            " * LOSS - 3.418, TOP-5 ACCURACY - 72.251, BLEU-4 - 0.23324059142117531\n",
            "\n",
            "Epoch: [9][0/126]\tLoss 3.4303 (3.4303)\tTop-5 Accuracy 74.212 (74.212)\n",
            "Epoch: [9][100/126]\tLoss 3.7275 (3.6645)\tTop-5 Accuracy 67.416 (68.874)\n",
            "Validation: [0/126]\tLoss 3.8402 (3.8402)\tTop-5 Accuracy 65.699 (65.699)\t\n",
            "Validation: [100/126]\tLoss 3.4193 (3.2776)\tTop-5 Accuracy 70.732 (74.398)\t\n",
            "\n",
            " * LOSS - 3.272, TOP-5 ACCURACY - 74.502, BLEU-4 - 0.2506592977906197\n",
            "\n",
            "Epoch: [10][0/126]\tLoss 3.4175 (3.4175)\tTop-5 Accuracy 75.775 (75.775)\n",
            "Epoch: [10][100/126]\tLoss 3.6446 (3.5355)\tTop-5 Accuracy 69.022 (70.551)\n",
            "Validation: [0/126]\tLoss 3.6524 (3.6524)\tTop-5 Accuracy 66.755 (66.755)\t\n",
            "Validation: [100/126]\tLoss 3.2698 (3.1360)\tTop-5 Accuracy 71.003 (76.122)\t\n",
            "\n",
            " * LOSS - 3.131, TOP-5 ACCURACY - 76.303, BLEU-4 - 0.27120788427299414\n",
            "\n",
            "Epoch: [11][0/126]\tLoss 3.2749 (3.2749)\tTop-5 Accuracy 73.961 (73.961)\n",
            "Epoch: [11][100/126]\tLoss 3.3782 (3.4182)\tTop-5 Accuracy 74.468 (72.289)\n",
            "Validation: [0/126]\tLoss 3.5097 (3.5097)\tTop-5 Accuracy 68.074 (68.074)\t\n",
            "Validation: [100/126]\tLoss 3.1421 (3.0101)\tTop-5 Accuracy 73.713 (77.988)\t\n",
            "\n",
            " * LOSS - 3.005, TOP-5 ACCURACY - 78.139, BLEU-4 - 0.2861279296066174\n",
            "\n",
            "Epoch: [12][0/126]\tLoss 3.4593 (3.4593)\tTop-5 Accuracy 70.845 (70.845)\n",
            "Epoch: [12][100/126]\tLoss 3.2358 (3.2965)\tTop-5 Accuracy 76.823 (74.042)\n",
            "Validation: [0/126]\tLoss 3.3667 (3.3667)\tTop-5 Accuracy 73.351 (73.351)\t\n",
            "Validation: [100/126]\tLoss 3.0207 (2.8891)\tTop-5 Accuracy 75.610 (79.759)\t\n",
            "\n",
            " * LOSS - 2.885, TOP-5 ACCURACY - 79.958, BLEU-4 - 0.3052062761842469\n",
            "\n",
            "Epoch: [13][0/126]\tLoss 3.2529 (3.2529)\tTop-5 Accuracy 74.366 (74.366)\n",
            "Epoch: [13][100/126]\tLoss 3.0269 (3.1850)\tTop-5 Accuracy 79.412 (75.494)\n",
            "Validation: [0/126]\tLoss 3.1852 (3.1852)\tTop-5 Accuracy 76.781 (76.781)\t\n",
            "Validation: [100/126]\tLoss 2.8945 (2.7694)\tTop-5 Accuracy 79.133 (82.096)\t\n",
            "\n",
            " * LOSS - 2.766, TOP-5 ACCURACY - 82.198, BLEU-4 - 0.32061753887216454\n",
            "\n",
            "Epoch: [14][0/126]\tLoss 2.9503 (2.9503)\tTop-5 Accuracy 80.508 (80.508)\n",
            "Epoch: [14][100/126]\tLoss 2.8912 (3.0796)\tTop-5 Accuracy 79.539 (77.234)\n",
            "Validation: [0/126]\tLoss 3.0996 (3.0996)\tTop-5 Accuracy 77.045 (77.045)\t\n",
            "Validation: [100/126]\tLoss 2.7931 (2.6727)\tTop-5 Accuracy 80.488 (83.387)\t\n",
            "\n",
            " * LOSS - 2.669, TOP-5 ACCURACY - 83.525, BLEU-4 - 0.3366370088318631\n",
            "\n",
            "Epoch: [15][0/126]\tLoss 2.9542 (2.9542)\tTop-5 Accuracy 78.889 (78.889)\n",
            "Epoch: [15][100/126]\tLoss 3.0286 (2.9924)\tTop-5 Accuracy 76.011 (78.501)\n",
            "Validation: [0/126]\tLoss 2.9690 (2.9690)\tTop-5 Accuracy 82.058 (82.058)\t\n",
            "Validation: [100/126]\tLoss 2.7033 (2.5731)\tTop-5 Accuracy 84.011 (86.076)\t\n",
            "\n",
            " * LOSS - 2.571, TOP-5 ACCURACY - 86.149, BLEU-4 - 0.3538338345989672\n",
            "\n",
            "Epoch: [16][0/126]\tLoss 2.7689 (2.7689)\tTop-5 Accuracy 83.014 (83.014)\n",
            "Epoch: [16][100/126]\tLoss 2.7695 (2.9049)\tTop-5 Accuracy 80.601 (79.962)\n",
            "Validation: [0/126]\tLoss 2.8292 (2.8292)\tTop-5 Accuracy 85.752 (85.752)\t\n",
            "Validation: [100/126]\tLoss 2.6067 (2.4850)\tTop-5 Accuracy 85.366 (87.566)\t\n",
            "\n",
            " * LOSS - 2.482, TOP-5 ACCURACY - 87.693, BLEU-4 - 0.3662397275222278\n",
            "\n",
            "Epoch: [17][0/126]\tLoss 2.7774 (2.7774)\tTop-5 Accuracy 82.535 (82.535)\n",
            "Epoch: [17][100/126]\tLoss 2.9363 (2.8019)\tTop-5 Accuracy 81.073 (81.962)\n",
            "Validation: [0/126]\tLoss 2.7260 (2.7260)\tTop-5 Accuracy 87.335 (87.335)\t\n",
            "Validation: [100/126]\tLoss 2.4696 (2.3821)\tTop-5 Accuracy 88.347 (89.369)\t\n",
            "\n",
            " * LOSS - 2.380, TOP-5 ACCURACY - 89.431, BLEU-4 - 0.38821559372145215\n",
            "\n",
            "Epoch: [18][0/126]\tLoss 2.8989 (2.8989)\tTop-5 Accuracy 79.221 (79.221)\n",
            "Epoch: [18][100/126]\tLoss 2.7442 (2.7228)\tTop-5 Accuracy 82.979 (83.360)\n",
            "Validation: [0/126]\tLoss 2.6470 (2.6470)\tTop-5 Accuracy 88.918 (88.918)\t\n",
            "Validation: [100/126]\tLoss 2.3920 (2.3092)\tTop-5 Accuracy 89.431 (90.834)\t\n",
            "\n",
            " * LOSS - 2.307, TOP-5 ACCURACY - 90.933, BLEU-4 - 0.4090197626347435\n",
            "\n",
            "Epoch: [19][0/126]\tLoss 2.6643 (2.6643)\tTop-5 Accuracy 85.674 (85.674)\n",
            "Epoch: [19][100/126]\tLoss 2.6021 (2.6499)\tTop-5 Accuracy 86.740 (84.696)\n",
            "Validation: [0/126]\tLoss 2.5419 (2.5419)\tTop-5 Accuracy 90.501 (90.501)\t\n",
            "Validation: [100/126]\tLoss 2.3090 (2.2437)\tTop-5 Accuracy 92.412 (92.012)\t\n",
            "\n",
            " * LOSS - 2.241, TOP-5 ACCURACY - 92.035, BLEU-4 - 0.4218404138944095\n",
            "\n",
            "Epoch: [20][0/126]\tLoss 2.7683 (2.7683)\tTop-5 Accuracy 82.857 (82.857)\n",
            "Epoch: [20][100/126]\tLoss 2.6645 (2.5714)\tTop-5 Accuracy 82.561 (86.152)\n",
            "Validation: [0/126]\tLoss 2.4724 (2.4724)\tTop-5 Accuracy 92.348 (92.348)\t\n",
            "Validation: [100/126]\tLoss 2.2518 (2.1680)\tTop-5 Accuracy 91.870 (93.046)\t\n",
            "\n",
            " * LOSS - 2.166, TOP-5 ACCURACY - 93.118, BLEU-4 - 0.4501754076735656\n",
            "\n",
            "Epoch: [21][0/126]\tLoss 2.3590 (2.3590)\tTop-5 Accuracy 89.011 (89.011)\n",
            "Epoch: [21][100/126]\tLoss 2.5180 (2.5030)\tTop-5 Accuracy 86.575 (87.334)\n",
            "Validation: [0/126]\tLoss 2.3425 (2.3425)\tTop-5 Accuracy 92.348 (92.348)\t\n",
            "Validation: [100/126]\tLoss 2.1513 (2.0819)\tTop-5 Accuracy 92.954 (94.100)\t\n",
            "\n",
            " * LOSS - 2.081, TOP-5 ACCURACY - 94.127, BLEU-4 - 0.47855929323157753\n",
            "\n",
            "Epoch: [22][0/126]\tLoss 2.2580 (2.2580)\tTop-5 Accuracy 91.413 (91.413)\n",
            "Epoch: [22][100/126]\tLoss 2.4476 (2.4383)\tTop-5 Accuracy 88.122 (88.333)\n",
            "Validation: [0/126]\tLoss 2.2532 (2.2532)\tTop-5 Accuracy 93.931 (93.931)\t\n",
            "Validation: [100/126]\tLoss 2.0816 (2.0144)\tTop-5 Accuracy 94.038 (94.750)\t\n",
            "\n",
            " * LOSS - 2.013, TOP-5 ACCURACY - 94.767, BLEU-4 - 0.4927544945606571\n",
            "\n",
            "Epoch: [23][0/126]\tLoss 2.4037 (2.4037)\tTop-5 Accuracy 90.349 (90.349)\n",
            "Epoch: [23][100/126]\tLoss 2.3416 (2.3656)\tTop-5 Accuracy 89.385 (89.535)\n",
            "Validation: [0/126]\tLoss 2.1928 (2.1928)\tTop-5 Accuracy 93.931 (93.931)\t\n",
            "Validation: [100/126]\tLoss 2.0334 (1.9654)\tTop-5 Accuracy 94.580 (95.291)\t\n",
            "\n",
            " * LOSS - 1.964, TOP-5 ACCURACY - 95.319, BLEU-4 - 0.5023874510014892\n",
            "\n",
            "Epoch: [24][0/126]\tLoss 2.1714 (2.1714)\tTop-5 Accuracy 93.199 (93.199)\n",
            "Epoch: [24][100/126]\tLoss 2.3207 (2.2941)\tTop-5 Accuracy 88.669 (90.470)\n",
            "Validation: [0/126]\tLoss 2.0919 (2.0919)\tTop-5 Accuracy 94.987 (94.987)\t\n",
            "Validation: [100/126]\tLoss 1.9595 (1.9026)\tTop-5 Accuracy 95.122 (96.147)\t\n",
            "\n",
            " * LOSS - 1.902, TOP-5 ACCURACY - 96.151, BLEU-4 - 0.5281714578353836\n",
            "\n",
            "Epoch: [25][0/126]\tLoss 2.1744 (2.1744)\tTop-5 Accuracy 91.477 (91.477)\n",
            "Epoch: [25][100/126]\tLoss 2.2043 (2.2493)\tTop-5 Accuracy 93.496 (91.296)\n",
            "Validation: [0/126]\tLoss 2.0510 (2.0510)\tTop-5 Accuracy 94.723 (94.723)\t\n",
            "Validation: [100/126]\tLoss 1.9256 (1.8509)\tTop-5 Accuracy 95.935 (96.331)\t\n",
            "\n",
            " * LOSS - 1.850, TOP-5 ACCURACY - 96.354, BLEU-4 - 0.5551175331893625\n",
            "\n",
            "Epoch: [26][0/126]\tLoss 2.3160 (2.3160)\tTop-5 Accuracy 90.625 (90.625)\n",
            "Epoch: [26][100/126]\tLoss 2.2113 (2.2046)\tTop-5 Accuracy 93.664 (92.019)\n",
            "Validation: [0/126]\tLoss 1.9860 (1.9860)\tTop-5 Accuracy 95.251 (95.251)\t\n",
            "Validation: [100/126]\tLoss 1.8497 (1.7960)\tTop-5 Accuracy 96.206 (97.059)\t\n",
            "\n",
            " * LOSS - 1.795, TOP-5 ACCURACY - 97.044, BLEU-4 - 0.5874029255268002\n",
            "\n",
            "Epoch: [27][0/126]\tLoss 2.0697 (2.0697)\tTop-5 Accuracy 92.443 (92.443)\n",
            "Epoch: [27][100/126]\tLoss 2.2275 (2.1498)\tTop-5 Accuracy 90.489 (92.548)\n",
            "Validation: [0/126]\tLoss 1.9228 (1.9228)\tTop-5 Accuracy 96.306 (96.306)\t\n",
            "Validation: [100/126]\tLoss 1.8133 (1.7501)\tTop-5 Accuracy 97.019 (97.490)\t\n",
            "\n",
            " * LOSS - 1.750, TOP-5 ACCURACY - 97.461, BLEU-4 - 0.6012749059546479\n",
            "\n",
            "Epoch: [28][0/126]\tLoss 2.0572 (2.0572)\tTop-5 Accuracy 94.385 (94.385)\n",
            "Epoch: [28][100/126]\tLoss 2.1860 (2.1068)\tTop-5 Accuracy 92.331 (93.053)\n",
            "Validation: [0/126]\tLoss 1.8452 (1.8452)\tTop-5 Accuracy 97.361 (97.361)\t\n",
            "Validation: [100/126]\tLoss 1.7653 (1.7071)\tTop-5 Accuracy 98.103 (97.939)\t\n",
            "\n",
            " * LOSS - 1.707, TOP-5 ACCURACY - 97.904, BLEU-4 - 0.6171666415822589\n",
            "\n",
            "Epoch: [29][0/126]\tLoss 1.9197 (1.9197)\tTop-5 Accuracy 95.308 (95.308)\n",
            "Epoch: [29][100/126]\tLoss 2.0674 (2.0412)\tTop-5 Accuracy 94.667 (94.031)\n",
            "Validation: [0/126]\tLoss 1.7977 (1.7977)\tTop-5 Accuracy 97.625 (97.625)\t\n",
            "Validation: [100/126]\tLoss 1.7065 (1.6678)\tTop-5 Accuracy 98.645 (98.234)\t\n",
            "\n",
            " * LOSS - 1.667, TOP-5 ACCURACY - 98.212, BLEU-4 - 0.6278443722978151\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}